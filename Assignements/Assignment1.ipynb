{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 (35 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Date posted__: Monday March 4\n",
    "\n",
    "__Date due__: Thursday March 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignement we will review the concepts that we have learned so far in class. This includes \n",
    "\n",
    "- Linear regression and regularization\n",
    "- Linear classification\n",
    "- Kernel methods\n",
    "\n",
    "In the first exercise, we will also cover some important ideas from optimization that will pave the way for the lecture on neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 : A walk through the energy landscape (7pts)\n",
    "\n",
    "As we have seen in class, except when there are very simple and can be solved explicitely, the parameters of most models in machine learning are set through optimization. As a first exercise, we will consider a simple model. Let us first consider the simple linear regression landscape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following function whose plot is given below. You can uncomment the first line to activate interactive 3D plot on jupyter and use the \"plt.ioff()\n",
    "%matplotlib inline\" cell below to turn the interactive mode off\n",
    "\n",
    "$$f(x,y) =  3(1-x)^2 e^{-(x^2) - (y+1)^2}- 10(x/5 - x^3 - y^5)e^{-x^2-y^2}- \\frac{1}{3} e^{-(x+1)^2 - y^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook # use to activate interactive plotting \n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-2,2,100)\n",
    "y = np.linspace(-3,3,100)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y, indexing='xy')\n",
    "\n",
    "z = 3*((1-xx)**2) * np.exp(-(xx**2) - (yy+1)**2) \\\n",
    "- 10*(xx/5 - xx**3 - yy**5) * np.exp(-xx**2 - yy**2)- (1/3)* np.exp(-(xx+1)**2 - yy**2)\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import axes3d    \n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, z, cmap=cm.viridis,alpha=.4,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1a. (4pts)__ Code a simple gradient algorithm. Set the parameters as follows\n",
    "\n",
    "- learning rate = step size: 0.1 \n",
    "- Max number of iterations: 20 \n",
    "- Stopping criterion: 0.0001 (Your iterations should stop when your gradient is smaller than the threshold)\n",
    "\n",
    "Then start your algorithm at \n",
    "\n",
    "- (x0 = 0.5, y0 = -0.5)\n",
    "- (x0 = -0.3, y0 = -0.3)\n",
    "\n",
    "And plot the iterations on top of the function (you can use the 3D plotting tools from matplolib or the simpler 'contourf' function from pyplot). What do you see ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1b.(3pts)__ Now consider the function \n",
    "\n",
    "$$f(x,y) = 0.5*(x^2)+0.25*(y^4)-0.5*(y^2)$$\n",
    "\n",
    "Plot this function using 'contourf' Then use your gradient descent algorithm with the same parameters as in __1.1.a__ and starting from the point (x0 = 1, y0 = 0). What do you observe? Choose any other initialization of the form $(x0, 0)$, same question? Explain this phenomenon ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (10 pts): Learning from the earth. \n",
    "\n",
    "We want to use your gradient descent algorithm to learn a regression model. Download the temperature data from Berkeley Earth (start with the \"All land monthly data\" \"http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_summary.txt\"). Save the corresponding text file in the folder of your notebook and use the lines below (Alternatively, you can just fork the whole repository on github). \n",
    "\n",
    "__Exercise 2a (3 pts)__: Use the snippet below to get the years and the temperatures stored as anomalies with respect to the average temperature (Celsius) of 8.64 +/- 0.10. Then use your gradient descent algorithm to learn a linear regression model of the form $ T(x) = \\beta_0 + \\beta_1 x$ where $x$ encodes the year and and $T(x)$ the temperature (anomalies). Plot your regression model on top of the data. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Complete_TAVG_summary.txt\", comment=\"%\")\n",
    "\n",
    "\n",
    "year = np.zeros((data.shape[0],1))\n",
    "temp = np.zeros((data.shape[0],1))\n",
    "\n",
    "\n",
    "for k in range(0, data.shape[0]):\n",
    "    \n",
    "    tmp = data.values[k]\n",
    "    tmp = tmp[0]\n",
    "    tmp = tmp.split()\n",
    "    tmp = map(float, tmp)\n",
    "    \n",
    "    year[k] = tmp[0]\n",
    "    temp[k] = tmp[1]\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.scatter(year, temp, s=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2b (3 pts)__: Repeat the experiments with the dataset from Sierra Leone (http://berkeleyearth.lbl.gov/auto/Regional/TAVG/Text/sierra-leone-TAVG-Trend.txt). Use the annual anomaly. Start in 1873 to avoid the Nan and average over the months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2c (4 pts)__: For both datasets, use the function pipeline from scikitlearn (see the documentation if you are unsure how to use it) and learn a regression model on monomials up to degree 4. (In this exercise you can use the LinearRegression function from Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (18pts):  Efficient face recognition with kernels\n",
    "In this third exercise, we will use the olivetti face dataset (this dataset can be loaded via scikit learn using the snippet below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "data = fetch_olivetti_faces()\n",
    "targets = data.target\n",
    "\n",
    "data = data.images.reshape((len(data.images), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 400 faces of size 64x64 pixels which are stored in the numpy array \"data\" of size $60$ x $64^2$. To display an image, you then have to reshape the rows of the array \"data\" into a 64x64 matrix. Consider the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4096)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0kP+P6Yq2w244e5nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(np.shape(data)) # size of image matrix\n",
    "\n",
    "image1 = data[1,:].reshape((64,64))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image1, cmap='gray') \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3a (2pts).__ The images are stored as rows of the 'data' matrix. To each row corresponds a label which represents a given ID (person) and which is stored in the vector 'targets'. In other words, the $i^{th}$ row of the matrix data is the picture of the personne represented by the ID given by targets[i]. Plots 5 faces from the same person. (use the subplot function from pyplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3b (1pts)__ Now that you can display the faces, we want to learn a classifier that will automatically label new pictures. \n",
    "\n",
    " - Start by set of images at random between a training and a test set. (Keep 10% of the images as test set). Split the 'targets' vector accordingly to keep the ID associated to your images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Exercise 3c (2pts)__\n",
    "- The images are big (much bigger than the actual number of training samples) so we will work with a kernel. To define the classifier we will use the exponential kernel \n",
    "\n",
    "$$K(x,x') = \\exp\\left(-\\frac{\\|x-x'\\|^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "Computing the $T$ by $T$ matrix of this kernel for your training images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n"
     ]
    }
   ],
   "source": [
    "innerProd = np.dot(data,data.T)\n",
    "\n",
    "print(np.shape(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3d (4pts)__\n",
    "\n",
    "We would like to get an intuition of the data. However as we only kept the kernel, we now only have access to similarities between the images and we can therefore not \"represent\" our points directly anymore. For a given similarity matrix $K(x,x')$, it is however possible to obtain a representation of the data on a 2D or 3D space which preserves similarity (i.e if the images are close, then the 2D or 3D points will also be close) as follows:\n",
    "\n",
    "- Take the eigendecomposition of the Kernel matrix (this will give you the eigenvalues $\\lambda_i$ (sorted in descending order) and the corresponding eigenvectors $\\boldsymbol v_i$)\n",
    "\n",
    "- Take the first 2 or 3 eigenvectors (depending on whether you want a 2D or 3D representation of your data)\n",
    "\n",
    "- The coordinates of the points (lets say $x_k$) in your 2D or 3D representation are given by $x_k = (v_1(k), v_2(k))$ (2D) or $x_k = (v_1(k), v_2(k), v_3(k))$ (3D) where $v_i(k)$ is the $k^{th}$ coordinate of the $i^{th}$ eigenvector. \n",
    "\n",
    "Use this approach to get 2D and 3D representation of your dataset. Plot it with 'scatter' from pyplot and use the numbers in the 'targets' vector to color your points so that different colors corresponds to different ID/persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How does the image dataset look like on this lower dimensional representations?\n",
    "\n",
    "__Exercise 3d (Bonus, +2pts)__ In your opinion, how many eigenvectors are need to represent most of the information from your data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3e (9pts)__\n",
    "\n",
    "We will now learn a linear classifier.\n",
    "\n",
    "- start by generating the binary vectors of labels. (turn the vector of targets in a matrix containing binary vectors representing each class)\n",
    "\n",
    "- We want to learn a classifier in feature space without computing the feature vectors $\\phi(x)$\n",
    "\n",
    "- Recall that the general formulation of a linear classifier in feature space reads as \n",
    "\n",
    "$$\\min_{\\boldsymbol \\beta} J(\\boldsymbol \\beta)  = \\min_{\\boldsymbol \\beta} \\frac{1}{2}\\sum_{n=1}^N \\left\\{\\boldsymbol \\beta^T \\phi(\\boldsymbol x_n) - t_n \\right\\}^2 + \\frac{\\lambda}{2}\\boldsymbol \\beta^T\\boldsymbol\\beta, \\quad (1)$$\n",
    "\n",
    "Where $\\boldsymbol \\beta$ is the vector of parameters. \n",
    "\n",
    "- __1.__ Compute the derivative with respect to $\\boldsymbol \\beta$ and set it to zero to find the expression for $\\boldsymbol \\beta$. \n",
    "- __2.__ Write this expression for $\\boldsymbol \\beta$ as $\\boldsymbol \\beta = \\boldsymbol \\Phi \\boldsymbol a$ where $\\boldsymbol \\Phi$ is the matrix whose $n^{th}$ row is given by the feature vector $\\phi(\\boldsymbol x_n)$ and give the corresponding expression of the vector $\\boldsymbol a$.\n",
    "- __3.__ Substitute the expression of $\\boldsymbol \\beta$ (i.e $\\boldsymbol \\beta = \\boldsymbol \\Phi \\boldsymbol a$) into $(1)$ You should get a new objective $J(\\boldsymbol a)$ which is now a function of $\\boldsymbol a$ and of $\\boldsymbol \\Phi \\boldsymbol \\Phi^T$.\n",
    "\n",
    "- __4__ Replace $\\boldsymbol \\Phi \\boldsymbol \\Phi^T$ with the kernel, i.e. $\\boldsymbol \\Phi \\boldsymbol \\Phi^T = K$ where $K$ is the matrix we used above \n",
    "\n",
    "$$K(i,j) = \\exp(-\\frac{\\|\\text{Image}_i -\\text{Image}_j\\|^2}{\\sigma^2})$$\n",
    "\n",
    "- __5__ To compute the classifier, use each of the following approaches (give the math details for the first two approaches)\n",
    "        \n",
    "  - Set the derivative of $J(\\boldsymbol a)$ to zero (using matrix derivatives) and      solve for $\\boldsymbol a$\n",
    "        \n",
    "  - Use your expression for $\\boldsymbol a$ (derived in step 2) and substitute the expression for $\\boldsymbol \\beta = \\boldsymbol\\Phi \\boldsymbol a$, then solve for $\\boldsymbol a$\n",
    "        \n",
    "  - Finally modify your gradient descent algorithm to find the minimum of $J(\\boldsymbol a)$ (How does the landscape look like in this case?)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the gradient descent code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression of your classifier can also be written with the Kernel matrix by starting from the classifier defined on the feature vector $\\phi(x)$ and $\\beta$ ans substituting for $\\boldsymbol \\beta = \\boldsymbol \\Phi \\boldsymbol a$ i.e.\n",
    "\n",
    "$$ y(x) = \\boldsymbol \\beta^T \\boldsymbol\\phi(\\boldsymbol x) =\\boldsymbol a^T \\boldsymbol \\Phi\\boldsymbol\\phi(\\boldsymbol x) = \\sum_{n\\in \\text{training}} a_n K(x_n, x)$$\n",
    "\n",
    "For any image from the test set, define the name of the person as the one corresponding to the largest entry in $y(x)$. Let $\\hat{n}_i$ be the predicted name, and $n_i$ be the true name (i.e the number stored in 'targets'). Compute the error as \n",
    "\n",
    "$$\\text{error} = \\frac{\\text{number of test images with $\\hat{n}_i\\neq n_i$}}{\\text{Total number of test images}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
