{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning (continued) + Intro to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Manifold learning (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first exercise, we will compare the power of typical classifiers before and after applying dimensionality reduction. Load the digit dataset from scikit learn by using the lines below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets)\n",
    "\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by splitting the dataset into a training and a test part (lets say 10% test set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__1.1.1 No dimensionality reduction__ Apply Kmeans (either your own implementation or the scikit-learn implementation) to the original dataset.  Compute the validation error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.1.2. Multidimensional Scaling__ Apply MDS to the dataset to get a mapping of the data to 2D. Then apply K means and compute the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Introduction to reinforcement learning with Open AI 'gym'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second exercise, you will get to apply what we have learned on reinforcement learning. To get some intuition on Reinforcement learning, we will turn to the gym library from openAI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first exercise, we will focus on the simple 'Taxi-v2' game which is a highly  simplified version of a self driving cab. The objective in this game is for the cab to pick up passenger at some location and drop them at the right destination. In this game, we basically want the rewards/penalties to work as follows\n",
    "\n",
    " - We want to give the agent a reward when it drops the passengers at the right location \n",
    " - We will give it a strong penalty when it drops a passenger at the wrong location\n",
    " - Finally, to motivate the agent to reach the destinations as fast as possible, we will add a small for each time step after which the agent hasn't reached the desired destination\n",
    " \n",
    " \n",
    "At any time step, our agent can take any of the following 6 actions: go south, go north, go east, go west, pick up a passenger or drop him/her off. \n",
    " \n",
    "The cab can pick up/drop off a passenger at any of the four locations marked 'R', 'G', 'Y' and 'B'. \n",
    " \n",
    " \n",
    "If we add one additional state corresponding to the passanger actually being in the car, this gives a state space of dimension $5\\times 5 \\times (4+1) \\times5$. In other at each time step $t$, the agent is in one of the $500$ states, with passenger being somewhere and having to go somewhere else, and it has to decide among the $6$ possible actions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'Taxi-v2' game, the walls (which the taxi cannot cross) are represented by the vertical bars. The columns represent possible pathways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gym we access the environment through the 'env' variable. In particular, one can use 'env' to perform the following actions\n",
    "\n",
    " - env.reset (reset the environement to a random state)\n",
    " - env.step(action): update the state of the environment. This line returns the following:\n",
    "     - The description of the environment\n",
    "     - The reward associated with the last action \n",
    "     - A parameter that indicates if the cab has successfully completed the pick up and drop off (i.e such an action is called an episode)\n",
    " - 'env.render', displays one frame of the environment\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To communicate with the agent, i.e set or get the state of the environment, we will use the \n",
    "functions \n",
    "\n",
    "   - env.encode \n",
    "   - env.s\n",
    "    \n",
    "The first call encodes the a 4-tuple describing a given environment state (taxi row, taxi column, passenger index, destination index) into a single integer corresponding to that particular state\n",
    "\n",
    "\n",
    "Play a little with the 'env.encode',  'env.s' and 'env.render' to display different states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To each state in the environment is also associated a reward table which contains, for every possible state, the probability to move to that state, the number encoding the state, the associated reward and a boolean variable which is true if the action has been taken and false otherwise. I.e the table has the form {action: [(probability, nextstate, reward, done)]}. You can display the table associated to any given state by using 'env.P[yourstateNumber]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2.1 No Reinforcement__ As a first exercise, we will try to solve the cab problem without reinforcement (i.e taking random actions until the cab dropped the passenger). To do this, write an infinite while loop which takes random actions until the variable 'done' from the reward table has been set to true. Keep track of a penalty variable which you increase by 1 each time the cab incurs a -10 reward. \n",
    "\n",
    "Store the variables 'State', 'Action' and 'Reward' as well as the current frame (use current_frame = env.render(mode='ansi')) throughout the iterations. (E.g store everything in a dictionnary 'frames')\n",
    "\n",
    "(hint: use env.action_space.sample() to sample some action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the evolution of your cab using your dictionnary 'frames' and the snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2.2 Part I: Training through Reinforcement__ We now turn to reinforcement learning. The first thing we will do is to learn a Q table which encodes how much we value each action the agent takes at any given state. In Q-learning (which is a particular choice of reinforcement learning), the value of each action is updated as \n",
    "\n",
    "$$Q(\\text{state}, \\text{action}) \\leftarrow  (1-\\alpha)Q(\\text{state}, \\text{action}) + \\alpha(\\text{reward} + \\gamma \\max_a Q(\\text{next state}, \\text{all actions}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the value of an action is updated from the previous value, the immediate reward associated to the action and \n",
    "the opportunities given by the action (i.e the maximum potential reward among future actions)\n",
    "\n",
    "The larger $\\gamma$, the more we weight long run behaviors. The smaller the value of $\\gamma$, the more we weight short term rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The Q-table is initialized to $0$ using the lines below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the (state, reward) space, we will write two nested loops. An outer for loop, lets say over 100000 iterations which runs over episodes (that is we change iterations when done has been set to True). In each iteration we reset the initial state at random using 'state = env.reset()'\n",
    "\n",
    "Then inside the for, we will write a while loop which runs until done is set to True. Inside this while loop, we will perform the following actions:\n",
    "    \n",
    "   - We will randomly choose between either \n",
    "        \n",
    "         1.Exploring the space : in this case, we simply take a random action with env.action_space.sample()\n",
    "         \n",
    "         2.Taking advantage of our previous knowledge. In this case we use the Q table to define our next action: np.argmax(q_table[state])\n",
    "          \n",
    "   - Then we make a step and return the corresponding state\n",
    "     \n",
    "     'next_state, reward, done, info = env.step(action)'\n",
    "\n",
    "   - Given the previous state, the next state, the reward, the table, update the table following the rule above.  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once you have trained the agent (i.e the Q table). Display the value of the table for a few states between 0 and 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2.3. Evaluation__ OK. Now that we have the Q-table, we can simply evaluate the agent (lets say on 100 episodes) and see how the reward evolves compared to the random case. Write a for loop over about 100 iterations. In each outer iteration, start with a random state, set done = False, then write a while loop until done = False, where for each iteration, you return the best action from the Q-table for the current state, then update the environement using that state. \n",
    "\n",
    "Also define a variable cost that you increment by 1 each time you get a -10 reward. \n",
    "\n",
    "You can save the frames and display them as in the random case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
